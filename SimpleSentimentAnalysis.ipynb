{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c87d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40247/2385109352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1992\u001b[0m         \"\"\"\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected state_dict to be dict-like, got {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "\n",
    "import pygame as gui\n",
    "import sys\n",
    "import torch\n",
    "import speech_recognition as sr\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "w,h = 1600,900\n",
    "white = (255,255,255)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 128\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = TransformerModel(2)\n",
    "model.load_state_dict(\"sentiment.pth\")\n",
    "model.eval()\n",
    "\n",
    "class TomCounts_ARI:\n",
    "\tdef __init__(self):\n",
    "\t\tgui.mixer.init()\n",
    "\t\tgui.init()\n",
    "\t\tgui.font.init()\n",
    "\t\tself.det = None\n",
    "\t\tself.list = []\n",
    "\t\tself.font = gui.font.Font(\"./quake.TTF\", 80)\n",
    "\t\tself.count = 0\n",
    "\t\tself.width = 1920\n",
    "\t\tself.height = 1080\n",
    "\t\tself.bgimg = gui.image.load('bg.png')\n",
    "\t\tself.logo = gui.image.load('logo.png')\n",
    "\t\tself.shinelogo = gui.image.load('bglogo.png')\n",
    "\t\tself.shinelogo = gui.transform.scale(self.shinelogo,(800,800))\n",
    "\t\tself.bgimg = gui.transform.scale(self.bgimg,(self.width//2-100,self.height))\n",
    "\t\tself.disp = gui.display.set_mode((self.width,self.height),0,0)\n",
    "\t\tgui.display.set_caption(\"SENTIMENT Tom\")\n",
    "\t\tself.img = gui.image.load(\"Speak/0001.jpg\")\n",
    "\t\tself.img = gui.transform.scale(self.img,(w+120,h+160))\n",
    "\t\tself.CAPTURE_ALL()\n",
    "    \n",
    "\tdef infer(self,text):\n",
    "\t    model.eval()\n",
    "\t    encoded_input = tokenizer.encode_plus(\n",
    "\t        text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "\t    input_ids = encoded_input['input_ids']\n",
    "\t    attention_mask = encoded_input['attention_mask']\n",
    "        \n",
    "\t    with torch.no_grad():\n",
    "\t        outputs = model(input_ids, attention_mask)\n",
    "\t        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "\t        return predicted_labels.item()\n",
    "\n",
    "\tdef recordAudio(self):\n",
    "\t\tr = sr.Recognizer()\n",
    "\t\tr.dynamic_energy_threshold = False\n",
    "\t\tr.energy_threshold = 480\n",
    "\t\twith sr.Microphone() as source:\n",
    "\t\t    r.adjust_for_ambient_noise(source=source)\n",
    "\t\t    print('Listening')\n",
    "\t\t    try:\n",
    "\t\t\t    audio = r.listen(source,timeout=4,phrase_time_limit=4)\n",
    "\t\t\t    data = r.recognize_google(audio)\n",
    "\t\t\t    print(self.infer(data))\n",
    "\t\t\t    \n",
    "\t\t    except:\n",
    "\t\t        print(\"ERROR\")\n",
    "\n",
    "\tdef blitForever(self,val=None):\n",
    "\t\tif(val!=None):\n",
    "\t\t\tif(val not in self.list):\n",
    "\t\t\t\tself.list = []\n",
    "\t\t\t\tself.list.append(val)\n",
    "\t\t\t\tself.playAudioARI(val)\n",
    "\t\telse:\n",
    "\t\t\tself.disp.blit(self.img,(560,0,0,0))\n",
    "\t\tself.disp.blit(self.bgimg,(0,0))\n",
    "\t\tself.disp.blit(self.shinelogo,(0,self.height//2-450))\n",
    "\t\tself.disp.blit(self.logo,(150,self.height//2-300))\n",
    "\t\ttext1 = self.font.render(\"TOM DETECTED \"+str(self.det), True, white)\n",
    "\t\ttext1Rect = text1.get_rect()\n",
    "\t\ttext1Rect.center = (w//2,h)\n",
    "\t\tself.blittext()\n",
    "\t\tgui.display.update()\n",
    "\n",
    "\n",
    "\tdef blittext(self):\n",
    "\t\ttext1 = self.font.render(\"TOM DETECTED \"+str(self.det), True, white)\n",
    "\t\ttext1Rect = text1.get_rect()\n",
    "\t\ttext1Rect.center = (w//2+600,h+40)\n",
    "\t\tself.disp.blit(text1,text1Rect)\n",
    " \n",
    "\tdef playAudioARI(self,op):\n",
    "\t\ti = 1\n",
    "\t\tself.det = op[9]\n",
    "\t\tgui.mixer.music.load(op)\n",
    "\t\tgui.mixer.music.play()\n",
    "\t\twhile(True):\n",
    "\t\t\timg = gui.image.load(\"Speak/0\"+str(i).zfill(3)+\".jpg\")\n",
    "\t\t\timg = gui.transform.scale(img,(w+120,h+160))\n",
    "\t\t\tself.disp.blit(img,(560,0,0,0))\n",
    "\t\t\ti+=1\n",
    "\t\t\tif(i==10):\n",
    "\t\t\t\treturn\n",
    "\t\t\tself.disp.blit(self.imgframe,(0,0,0,0))\n",
    "\t\t\tself.blittext()\n",
    "\t\t\tgui.display.update()\n",
    "\n",
    "\n",
    "\tdef CAPTURE_ALL(self):\n",
    "\t\tRUN = True\n",
    "\t\twhile RUN:\n",
    "\t\t\tself.blitForever()\n",
    "\t\t\tfor\teve in gui.event.get():\n",
    "\t\t\t\tif eve.type==gui.KEYDOWN:\n",
    "\t\t\t\t\tif eve.key==gui.K_RETURN:\n",
    "\t\t\t\t\t\tself.infer(\"I am good\")\n",
    "\t\t\t\t\telif eve.key==gui.K_ESCAPE:\n",
    "\t\t\t\t\t\tRUN = False\n",
    "\t\t\t\t\t\tgui.quit()\n",
    "\t\t\t\t\t\treturn\n",
    "\n",
    "TomCounts_ARI()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
